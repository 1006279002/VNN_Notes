损失函数是衡量**模型输出**与**真实标签**之间的差异
* 损失函数 计算**一个**样本的模型输出与真实标签的差异$Loss=f(\hat{y},y)$
* 代价函数 计算**整个**样本集的模型输出与真是标签的差异，是所有样本损失函数的平均值$cost=\frac{1}{N}\sum^N_if(\hat{y_i},y_i)$
* 目标函数 代价函数加上**正则项**

PyTorch中损失函数继承于`nn.Module`，相当于一个网络层

**注意**: 在所有的损失函数中，`size_average`和`reduce`参数都不再使用

### nn.CrossEntropyLoss
```python
nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
# 把nn.LogSoftmax()和nn.NLLLoss()结合，计算交叉熵，前者将输出归一化到了(0,1)区间
```
- `weight`：各类别的 loss 设置权值
- `ignore_index`：忽略某个类别的 loss 计算
- `reduction`：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

介绍一下**熵的基本概念**
* 自信息 $I(x)=-log[p(x)]$
* 信息熵就是求自信息的期望 $H(P)=E_{x\sim p}[I(x)]=-\sum_i^NP(x_i)logP(x_i)$
* 相对熵，也被称为KL散度，用于衡量两个分布的相似性$$D_{KL}(P,Q)=E_{x\sim p}[log\frac{P(x)}{Q(x)}]$$其中$P(x)$是**真实的分布**，$Q(x)$是**拟合的分布** ^8cef9b
* 交叉熵 $H(P,Q)=-\sum_{i=1}^NP(x_i)logQ(x_i)$

对相对熵进行展开，可以得到$$\begin{aligned}D_{KL}(P,Q)&=E_{x\sim p}[log\frac{P(x)}{Q(x)}]\\&=E_{x\sim p}[log{P(x)}-log{Q(x)}]\\&=\sum_{i=1}^NP(x_i)[logP(x_i)-logQ(x_i)]\\&=(-\sum_{i=1}^NP(x_i)logQ(x_i))-(-\sum_{i=1}^NP(x_i)logP(x_i))\\&=H(P,Q)-H(P) \end{aligned}$$即，交叉熵=信息熵+相对熵，$H(P,Q)=D_{KL}(P,Q)+H(P)$，由于信息熵是固定的，所以优化交叉熵等价于优化相对熵

由此得到**每一个样本**的交叉熵计算公式$$H(P,Q)=-\sum_{i=1}^NP(x_i)logQ(x_i)=-logQ(x_i)$$
$$loss(x,class)=weight[class](-x[class]+log(\sum_jexp(x[j])))$$
* `class`指的是具体的**目标分类**，二分类中不是0就是1
* `weight`指权重，不设置就默认为1

示例代码如下
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# fake data
inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], dtype=torch.float)
target = torch.tensor([0, 1, 1], dtype=torch.long)

# def loss function
loss_f_none = nn.CrossEntropyLoss(weight=None, reduction='none')
loss_f_sum = nn.CrossEntropyLoss(weight=None, reduction='sum')
loss_f_mean = nn.CrossEntropyLoss(weight=None, reduction='mean')

# forward
loss_none = loss_f_none(inputs, target)
loss_sum = loss_f_sum(inputs, target)
loss_mean = loss_f_mean(inputs, target)

# view
print("Cross Entropy Loss:\n ", loss_none, loss_sum, loss_mean)
```

结果如下
```text
Cross Entropy Loss:
  tensor([1.3133, 0.1269, 0.1269]) tensor(1.5671) tensor(0.5224)
```

### nn.NLLLoss
```python
nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
# 实现负对数似然函数中的符号功能
```
- `weight`：各类别的 loss 权值设置
- `ignore_index`：忽略某个类别
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

每个样本的loss计算公式为$l_n=-w_{y_n}x_{n,y_n}$(其中的$y_n$指代的是class)

代码示例
```python
weights = torch.tensor([1, 1], dtype=torch.float)

loss_f_none_w = nn.NLLLoss(weight=weights, reduction='none')
loss_f_sum = nn.NLLLoss(weight=weights, reduction='sum')
loss_f_mean = nn.NLLLoss(weight=weights, reduction='mean')

# forward
loss_none_w = loss_f_none_w(inputs, target)
loss_sum = loss_f_sum(inputs, target)
loss_mean = loss_f_mean(inputs, target)

# view
print("\nweights: ", weights)
print("NLL Loss", loss_none_w, loss_sum, loss_mean)
```

### nn.BCELoss
```python
nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')
# 计算二分类的交叉熵，输出区间为[0,1]
```
- `weight`：各类别的 loss 权值设置
- `ignore_index`：忽略某个类别
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式为$l_n=-w_n[y_n\cdot logx_n+(1-y_n)\cdot log(1-x_n)]$
* 预测的标签需要经过sigmoid变换到`[0,1]`之间
* **真实的标签**需要转换为one hot向量，类型为`torch.float`

代码如下
```python
inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)
target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)

target_bce = target

# itarget
inputs = torch.sigmoid(inputs)

weights = torch.tensor([1, 1], dtype=torch.float)

loss_f_none_w = nn.BCELoss(weight=weights, reduction='none')
loss_f_sum = nn.BCELoss(weight=weights, reduction='sum')
loss_f_mean = nn.BCELoss(weight=weights, reduction='mean')

# forward
loss_none_w = loss_f_none_w(inputs, target_bce)
loss_sum = loss_f_sum(inputs, target_bce)
loss_mean = loss_f_mean(inputs, target_bce)

# view
print("\nweights: ", weights)
print("BCE Loss", loss_none_w, loss_sum, loss_mean)
```

### nn.BCEWithLogitsLoss
```python
nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)
# 结合sigmoid与二分类交叉熵，网络最后不需要再经过sigmoid
```
- `weight`：各类别的 loss 权值设置
- `pos_weight`：**设置样本类别对应的神经元的输出的 loss 权值**
- `ignore_index`：忽略某个类别
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

代码如下
```python
inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], dtype=torch.float)
target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], dtype=torch.float)

target_bce = target

# itarget
# inputs = torch.sigmoid(inputs)

weights = torch.tensor([1], dtype=torch.float)
pos_w = torch.tensor([3], dtype=torch.float)        # 3

loss_f_none_w = nn.BCEWithLogitsLoss(weight=weights, reduction='none', pos_weight=pos_w)
loss_f_sum = nn.BCEWithLogitsLoss(weight=weights, reduction='sum', pos_weight=pos_w)
loss_f_mean = nn.BCEWithLogitsLoss(weight=weights, reduction='mean', pos_weight=pos_w)

# forward
loss_none_w = loss_f_none_w(inputs, target_bce)
loss_sum = loss_f_sum(inputs, target_bce)
loss_mean = loss_f_mean(inputs, target_bce)

# view
print("\npos_weights: ", pos_w)
print(loss_none_w, loss_sum, loss_mean)
```

### nn.L1Loss
```python
nn.L1Loss(size_average=None, reduce=None, reduction='mean')
# 计算差值绝对值
```
* `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

公式 $l_n=|x_n-y_n|$

### nn.MSELoss
```python
nn.MSELoss(size_average=None, reduce=None, reduction='mean')
# 计算差方
```
* `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

公式$l_n=(x_n-y_n)^2$

### nn.SmoothL1Loss
```python
nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')
# 更平滑的L1Loss
```
* `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

公式$$z_i=\begin{cases}0.5(x_i-y_i)^2&\text{if $|x_i-y_i|<1$}\\|x_i-y_i|-0.5&\text{otherwise}\end{cases}$$

在接近0的地方**可微**，更平滑

### nn.PoissonNLLLoss
```python
nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')
# 泊松分布的负对数似然损失函数
```
- `log_input`：输入是否为对数形式，决定计算公式
	- 当 `log_input = True`，表示输入数据已经是经过对数运算之后的，$loss(input, target) = exp(input) - target * input$
	- 当 `log_input = False`，，表示输入数据还没有取对数，$loss(input, target) = input - target * log(input+eps)$
- `full`：计算所有 loss，默认为 False
- `eps`：修正项，避免 log(input) 为 nan

代码如下
```python
inputs = torch.randn((2, 2))
target = torch.randn((2, 2))

loss_f = nn.PoissonNLLLoss(log_input=True, full=False, reduction='none')
loss = loss_f(inputs, target)
print("input:{}\ntarget:{}\nPoisson NLL loss:{}".format(inputs, target, loss))
```

### nn.KLDivLoss
```python
nn.KLDivLoss(size_average=None, reduce=None, reduction='mean')
# 计算KLD，KL散度，相对熵
# 如果通过nn.logsoftmax则需要提前将输入计算log-probabilities
```

[[#^8cef9b|公式]]就在上文

对每个样本来说，其计算公式应该是$l_n=y_n\cdot(logy_n-x_n)$，其中$y_n$是真实值$P(x)$，$x_n$是对数运算之后的预测值$logQ(x)$

代码如下
```python
inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])
inputs_log = torch.log(inputs)
target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)

loss_f_none = nn.KLDivLoss(reduction='none')
loss_f_mean = nn.KLDivLoss(reduction='mean')
loss_f_bs_mean = nn.KLDivLoss(reduction='batchmean')

loss_none = loss_f_none(inputs, target)
loss_mean = loss_f_mean(inputs, target)
loss_bs_mean = loss_f_bs_mean(inputs, target)

print("loss_none:\n{}\nloss_mean:\n{}\nloss_bs_mean:\n{}".format(loss_none, loss_mean, loss_bs_mean))
```

### nn.MarginRankingLoss
```python
nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')
# 计算两个向量之间的相似程度，用于排序任务
# 返回一个n*n的矩阵
```
- `margin`：边界值，$x_1$ 与 $x_2$ 之间的差异值
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式$$loss(x,y)=max(0,-y*(x_1-x_2)+margin)$$其中$y$的取值只有$+1$和$-1$

代码如下
```python
x1 = torch.tensor([[1], [2], [3]], dtype=torch.float)
x2 = torch.tensor([[2], [2], [2]], dtype=torch.float)

target = torch.tensor([1, 1, -1], dtype=torch.float)

loss_f_none = nn.MarginRankingLoss(margin=0, reduction='none')

loss = loss_f_none(x1, x2, target)

print(loss)
```

### nn.MultiLabelMarginLoss
```python
nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')
# 多标签边界损失函数
```

举例：4 分类任务，样本 x 属于 0 类和 3 类，那么标签为 \[0, 3, -1, -1]

计算公式$$loss(x,y)=\sum_{i,j}\frac{max(0,1-(x[y[j]]-x[i]))}{x-size(0)}$$

代码如下
```python
x = torch.tensor([[0.1, 0.2, 0.4, 0.8]])
y = torch.tensor([[0, 3, -1, -1]], dtype=torch.long)

loss_f = nn.MultiLabelMarginLoss(reduction='none')

loss = loss_f(x, y)

print(loss)
```

### nn.SoftMarginLoss
```python
nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')
# 计算二分类的logistic损失
```

计算公式$$loss(x,y)=\sum_i\frac{log(1+exp(-y[i]*x[i]))}{x.nelement\ 0}$$

代码如下
```python
inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])
target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)

loss_f = nn.SoftMarginLoss(reduction='none')

loss = loss_f(inputs, target)

print("SoftMargin: ", loss)
```

### nn.MultiLabelSoftMarginLoss
```python
nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')
# SoftMarginLoss的多标签版本
```
- `weight`：各类别的 loss 权值设置
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式$$loss(x,y)=-\frac{1}{C}*\sum_iy[i]*log((1+exp(-x[i]))^{-1})+(1-y[i]*log(\frac{exp(-x[i])}{1+exp(-x[i])}))$$

代码如下
```python
inputs = torch.tensor([[0.3, 0.7, 0.8]])
target = torch.tensor([[0, 1, 1]], dtype=torch.float)

loss_f = nn.MultiLabelSoftMarginLoss(reduction='none')

loss = loss_f(inputs, target)

print("MultiLabel SoftMargin: ", loss)
```

### nn.MultiMarginLoss
```python
nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')
# 计算多分类的折页损失
```
- `p`：可以选择 1 或 2
- `weight`：各类别的 loss 权值设置
- `margin`：边界值
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式$$loss(x,y)=\frac{\sum_imax(0,margin-x[y]+x[i])^p}{x.size(0)}$$

代码如下
```python
x = torch.tensor([[0.1, 0.2, 0.7], [0.2, 0.5, 0.3]])
y = torch.tensor([1, 2], dtype=torch.long)

loss_f = nn.MultiMarginLoss(reduction='none')

loss = loss_f(x, y)

print("Multi Margin Loss: ", loss)
```

### nn.TripletMarginLoss
```python
nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')
# 计算三元组损失，常用于人脸验证
```
- `p`：范数的阶，默认为 2
- `margin`：边界值
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式$$L(a,p,n)=max\{d(a_i,p_i)-d(a_i,n_i)+margin,0\},d(x_i,y_i)=\lVert x_i-y_i\rVert_p$$

代码示例
```python
anchor = torch.tensor([[1.]])
pos = torch.tensor([[2.]])
neg = torch.tensor([[0.5]])

loss_f = nn.TripletMarginLoss(margin=1.0, p=1)

loss = loss_f(anchor, pos, neg)

print("Triplet Margin Loss", loss)
```

### nn.HingeEmbeddingLoss
```python
nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')
# 计算两个输入的相似性，常用于非线性embedding和半监督学习
# 输入 x 应该为两个输入之差的绝对值
```
- `margin`：边界值
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式$$l_n=\begin{cases}x_n&\text{if $y_n=1$}\\max\{0,\Delta-x_n\}&\text{if $y_n=-1$}\end{cases}$$

代码如下
```python
inputs = torch.tensor([[1., 0.8, 0.5]])
target = torch.tensor([[1, 1, -1]])

loss_f = nn.HingeEmbeddingLoss(margin=1, reduction='none')

loss = loss_f(inputs, target)

print("Hinge Embedding Loss", loss)
```

### nn.CosineEmbeddingLoss
```python
torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')
# 采用余弦相似度计算两个输入的相似性
```
- `margin`：边界值，可取值 \[-1, 1]，推荐为 \[0, 0.5]
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)

计算公式$$loss(x,y)=\begin{cases}1-cos(x_1,x_2)&\text{if $y=1$}\\ max\{0,cos(x_1,x_2)-margin\}&\text{if $y=-1$} \end{cases}$$其中$cos(\theta)=\frac{A\cdot B}{\lVert A\rVert \lVert B\rVert}=\frac{\sum_{i=1}^nA_i\times B_i}{\sqrt{\sum_{i=1}^n(A_i)^2}\times\sqrt{\sum_{i=1}^n(B_i)^2}}$

代码示例
```python
x1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])
x2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])

target = torch.tensor([[1, -1]], dtype=torch.float)

loss_f = nn.CosineEmbeddingLoss(margin=0., reduction='none')

loss = loss_f(x1, x2, target)

print("Cosine Embedding Loss", loss)
```

### nn.CTCLoss
```python
nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)
# 计算 CTC 损失，解决时序类数据的分类，全称为 Connectionist Temporal Classification
```
- `blank`：blank label
- `zero_infinity`：无穷大的值或梯度置 0
- `reduction`：计算模式，，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)


