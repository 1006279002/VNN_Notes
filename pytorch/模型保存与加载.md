### 序列化与反序列化
模型在内存以对象的逻辑结构保存的，但是在硬盘中是以二进制流的方式保存的
- 序列化是指将内存中的数据以二进制序列的方式保存到硬盘中。PyTorch 的模型保存就是序列化。  
- 反序列化是指将硬盘中的二进制序列加载到内存中，得到模型的对象。PyTorch 的模型加载就是反序列化。

### PyTorch中的模型保存和加载
#### torch.save
```python
torch.save(obj, f, pickle_module, pickle_protocol=2, _use_new_zipfile_serialization=False)
```
- `obj`：保存的对象，可以是模型。也可以是 dict。因为一般在保存模型时，不仅要保存模型，还需要保存优化器、此时对应的 epoch 等参数。这时就可以用 dict 包装起来。
- `f`：输出路径

保存还存在两种方式

##### 保存整个Module
```python
# 这种方法比较耗时，保存的文件大
torch.save(net, path)
```

##### 只保存模型的参数
```python
# 运行比较快，保存的文件比较小
# 首推
state_sict = net.state_dict()
torch.save(state_sict, path)
```

#### torch.load
```python
torch.load(f, map_location=None, pickle_module, **pickle_load_args)
```
- `f`：文件路径
- `map_location`：指定存在 CPU 或者 GPU

加载也存在两种方式

##### 加载整个Module
如果保存的时候，保存的是整个模型，那么加载时就加载整个模型。这种方法不需要事先创建一个模型对象，也不用知道模型的结构

直接加载具体pkl文件即可

##### 只加载模型的参数
如果保存的时候，保存的是模型的参数，那么加载时就参数。这种方法需要事先创建一个模型对象，再使用模型的`load_state_dict()`方法把参数加载到模型中

### 模型的断点续训练
在训练过程中，可能由于某种意外原因如断点等导致训练终止，这时需要重新开始训练。断点续练是在训练过程中每隔一定次数的 epoch 就保存**模型的参数和优化器的参数**，这样如果意外终止训练了，下次就可以重新加载最新的**模型参数和优化器的参数**，在这个基础上继续训练

例如下面代码
```python
	# 每隔checkpoint_interval个数个epoch就进行一次保存
    if (epoch+1) % checkpoint_interval == 0:

        checkpoint = {"model_state_dict": net.state_dict(),
                      "optimizer_state_dict": optimizer.state_dict(),
                      "epoch": epoch}
        path_checkpoint = "./checkpoint_{}_epoch.pkl".format(epoch)
        torch.save(checkpoint, path_checkpoint)
```

读取代码示例
```python
path_checkpoint = "./checkpoint_4_epoch.pkl"
checkpoint = torch.load(path_checkpoint)

net.load_state_dict(checkpoint['model_state_dict'])

optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

start_epoch = checkpoint['epoch']

scheduler.last_epoch = start_epoch
```

需要注意的是，读取的时候还要设置`scheduler.last_epoch`参数为保存的 epoch。模型训练的起始 epoch 也要修改为保存的 epoch
