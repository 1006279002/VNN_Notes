### 静态计算图
计算图的概念可以移步[[../Computational Graph/计算图|计算图]]

为了高效计算，pytorch也应用了计算图设计，下面针对一个实际例子进行分析学习

针对函数$$y=(x+w)*(w+1)$$可以获得如下计算图<br>
![pytorch计算图示例](../Excalidraw/pytorch计算图示例)

计算图规则就是**不同路径相加，相同路径相乘**，就可以快速计算偏导了

针对上面示例的代码示例如下
```python
import torch
w = torch.tensor([1.], requires_grad=True)
x = torch.tensor([2.], requires_grad=True)
# y=(x+w)*(w+1)
a = torch.add(w, x)     # retain_grad()
b = torch.add(w, 1)
y = torch.mul(a, b)
# y 求导
y.backward()
# 打印 w 的梯度，就是 y 对 w 的导数
print(w.grad)
```

按照[[Tensor介绍#^e44ffe|这个属性]]`is_leaf`可知此tensor是否为叶子节点，同时叶子节点是所有节点中最重要的节点，**反向传播**计算结束后，非叶子节点的梯度会被自动释放，节省内存

代码示例
```python
# 查看叶子结点
print("is_leaf:\n", w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)

# 查看梯度
print("gradient:\n", w.grad, x.grad, a.grad, b.grad, y.grad)
```

结果如下
```text
is_leaf:
 True True False False False
gradient:
 tensor([5.]) tensor([2.]) None None None
```

同时也会收到pytorch的警告，不要随便调用不存在的梯度

与此同时，`grad_fn`[[Tensor介绍#^50db12|这个属性]]记录了创建该张量时的方法，反向求导时需要用到该属性

代码示例
```python
# 查看梯度
print("w.grad_fn = ", w.grad_fn)
print("x.grad_fn = ", x.grad_fn)
print("a.grad_fn = ", a.grad_fn)
print("b.grad_fn = ", b.grad_fn)
print("y.grad_fn = ", y.grad_fn)
```

结果如下
```text
w.grad_fn =  None
x.grad_fn =  None
a.grad_fn =  <AddBackward0 object at 0x0000026EDD31F430>
b.grad_fn =  <AddBackward0 object at 0x0000026EDD31F430>
y.grad_fn =  <MulBackward0 object at 0x0000026EDD31F430>
```

### pytorch的动态图机制
PyTorch采用的是**动态图机制**，而Tensorflow采用的是**静态图机制**

动态图就是**运算和搭建同时进行**，灵活，易调节，易测试

静态图就是**先搭建图**，然后再根据输入的数据进行运算，**运行速度更快**，每次运行的图都会是一样的，无法改变

