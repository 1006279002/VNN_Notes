适当的权值初始化可以加快模型的收敛，而不恰当的权值初始化可能引发**梯度消失**或者**梯度爆炸**，最终导致模型无法收敛。

### 梯度消失与梯度爆炸
考虑一个三层的全连接网络$$H_1=X\times W_1,H_2=H_1\times W_2,Out=H_2\times W_3$$
针对第二层计算梯度可以得到$$\begin{aligned}\Delta W_2&=\frac{\partial Loss}{\partial W_2}=\frac{\partial Loss}{\partial Out}\frac{\partial Out}{\partial H_2}\frac{\partial H_2}{\partial W_2}\\&=\frac{\partial Loss}{\partial Out}\frac{\partial Out}{\partial H_2}H_1\end{aligned}$$
所以 $\Delta W_2$ 依赖于前一层的输出 $H_1$。如果 $H_1$ 趋近于零，那么 $\Delta W_2$ 也接近于 0，造成梯度消失。如果 $H_1$ 趋近于无穷大，那么 $\Delta W_2$ 也接近于无穷大，造成梯度爆炸。

下面通过一个简单的代码示例来掩饰梯度消失/梯度爆炸
```python
import torch
import torch.nn as nn
from common_tools import set_seed

set_seed(1)  # 设置随机种子


class MLP(nn.Module):
    def __init__(self, neural_num, layers):
        super(MLP, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=False) for i in range(layers)])
        self.neural_num = neural_num

    def forward(self, x):
        for (i, linear) in enumerate(self.linears):
            x = linear(x)


        return x

    def initialize(self):
        for m in self.modules():
            # 判断这一层是否为线性层，如果为线性层则初始化权值
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight.data)    # normal: mean=0, std=1

layer_nums = 100
neural_nums = 256
batch_size = 16

net = MLP(neural_nums, layer_nums)
net.initialize()

inputs = torch.randn((batch_size, neural_nums))  # normal: mean=0, std=1

output = net(inputs)
print(output)
```

输出结果
```text
tensor([[nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        ...,
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MmBackward0>)
```

这次在`forward`中加入一个判断，如果出现nan就停止向前传播
```python
    def forward(self, x):
        for (i, linear) in enumerate(self.linears):
            x = linear(x)

            print("layer:{}, std:{}".format(i, x.std()))
            if torch.isnan(x.std()):
                print("output is nan in {} layers".format(i))
                break

        return x
```

输出如下
```text
layer:0, std:15.959932327270508
layer:1, std:256.6237487792969
layer:2, std:4107.24560546875
.
.
.
layer:28, std:8.150925520353362e+34
layer:29, std:1.322983152787379e+36
layer:30, std:nan
output is nan in 30 layers
tensor([[ 9.8052e+36,  2.5191e+37,  8.4358e+36,  ...,  4.1859e+37,
         -2.3880e+37, -1.1118e+37],
        [-3.6358e+37,  4.5755e+35, -2.7716e+36,  ..., -1.8793e+37,
          4.1133e+36, -1.2764e+37],
        [ 2.1538e+37, -3.1103e+37,  2.5804e+37,  ...,  6.9849e+36,
          3.2139e+37,  4.8494e+36],
        ...,
        [ 1.3798e+37,  7.6824e+36, -2.9655e+36,  ...,  8.7788e+35,
          1.3106e+37,  6.6469e+36],
        [ 1.2969e+37,  2.3706e+37, -1.0296e+37,  ...,  1.5095e+37,
         -3.8905e+37, -1.1750e+37],
        [-8.2960e+36, -8.1296e+36, -7.4200e+36,  ..., -1.9674e+37,
         -1.5635e+37,  1.5640e+36]], grad_fn=<MmBackward0>)
```

由此可见标准差逐层不断变大

通过方差公式可以推导出$$D(X\times Y)=D(X)\times D(Y)+D(X)\times[E(Y)]^2+D(Y)\times[E(X)]^2$$对$E(X)=E(Y)=0$的情况，针对第一个神经元，有$$H_{11}=\sum_{i=0}^nX_i\times W_{1i}$$其中输入$X$和权值$W$都是服从$N(0,1)$的正态分布，那么这个神经元的方差就会变成$$\begin{aligned}D(H_{11})&=\sum_{i=0}^nD(X_i)\times D(W_{1i})\\&=n*(1*1)\\&=1\end{aligned}$$
由此，每经过一个网络层，方差会扩大$\sqrt{n}$倍，那么如果将$W$的方差修改为$\sqrt{\frac{1}{n}}$，就可以解决这个问题，将代码修改为`nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_num))`，结果如下
```text
layer:0, std:0.9974957704544067
layer:1, std:1.0024365186691284
.
.
.
layer:98, std:1.1617802381515503
layer:99, std:1.2215303182601929
tensor([[-1.0696, -1.1373,  0.5047,  ..., -0.4766,  1.5904, -0.1076],
        [ 0.4572,  1.6211,  1.9659,  ..., -0.3558, -1.1235,  0.0979],
        [ 0.3908, -0.9998, -0.8680,  ..., -2.4161,  0.5035,  0.2814],
        ...,
        [ 0.1876,  0.7971, -0.5918,  ...,  0.5395, -0.8932,  0.1211],
        [-0.0102, -1.5027, -2.6860,  ...,  0.6954, -0.1858, -0.8027],
        [-0.5871, -1.3739, -2.9027,  ...,  1.6734,  0.5094, -0.9986]],
       grad_fn=<MmBackward0>)
```

但是，如果在`forward()`中增加了非线性变换`ReLU`，甚至方差会越来越小
```text
layer:0, std:0.584515392780304
layer:1, std:0.4393407702445984
layer:2, std:0.3229506015777588
.
.
.
layer:98, std:8.264128301259009e-16
layer:99, std:5.922812768017875e-16
```

### Xavier方法和Kaiming方法
#### Xavier方法
通过推导计算可以得出，需要满足$D(W)=\frac{2}{n_i+n_{i+1}}$，假设$W$服从均匀分布$U[-a,a]$，那么方差就可以计算得到$D(W)=\frac{a^2}{3}$，通过计算就可以得出$$a=\frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}$$
根据公式修改代码为
```python
a = np.sqrt(6 / (self.neural_num + self.neural_num))
# 把 a 变换到 tanh，计算增益
tanh_gain = nn.init.calculate_gain('tanh')
a *= tanh_gain

nn.init.uniform_(m.weight.data, -a, a)
```


得到输出
```text
layer:0, std:0.7571136355400085
layer:1, std:0.6924336552619934
layer:2, std:0.6677976846694946
.
.
.
layer:98, std:0.6407480835914612
layer:99, std:0.6442216038703918
tensor([[ 0.1159,  0.1230,  0.8216,  ...,  0.9417, -0.6332,  0.5106],
        [-0.9586, -0.2355,  0.8550,  ..., -0.2347,  0.9330,  0.0114],
        [ 0.9488, -0.2261,  0.8736,  ..., -0.9594,  0.7923,  0.6266],
        ...,
        [ 0.7160,  0.0916, -0.4326,  ..., -0.9586,  0.2504,  0.5406],
        [-0.9539,  0.5055, -0.8024,  ..., -0.4472, -0.6167,  0.9665],
        [ 0.6117,  0.3952,  0.1042,  ...,  0.3919, -0.5273,  0.0751]],
       grad_fn=<TanhBackward0>)
```

也可以直接调用PyTorch内部的Xavier方法
```python
tanh_gain = nn.init.calculate_gain('tanh')
nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)
```

#### nn.init.calculate_gain()
上述初始化都使用了`tanh_gain = nn.init.calculate_gain('tanh')`

```python
nn.init.calculate_gain(nonlinearity,param=**None**)
# 经过一个分布的方差，在经过激活函数后的变化尺度
```
- `nonlinearity`：激活函数名称
- `param`：激活函数的参数，如 Leaky ReLU 的 negative_slop

代码示例
```python
x = torch.randn(10000)
out = torch.tanh(x)

gain = x.std() / out.std()
print('gain:{}'.format(gain))

tanh_gain = nn.init.calculate_gain('tanh')
print('tanh_gain in PyTorch:', tanh_gain)
```

输出如下
```text
gain:1.5982500314712524
tanh_gain in PyTorch: 1.6666666666666667
```

#### Kaiming方法
由于tanh的使用开始减少，转而大部分开始使用ReLU，所以应对方式开始转变，针对ReLU，方差应该满足$D(W)=\frac{2}{n_i}$，标准差则是要满足$std(W)=\sqrt{\frac{2}{n_i}}$，直接使用`nn.init.kaiming_normal_(m.weight.data)`初始化即可

### 常用初始化方法
1. Xavier 均匀分布
2. Xavier 正态分布
3. Kaiming 均匀分布
4. Kaiming 正态分布
5. 均匀分布
6. 正态分布
7. 常数分布
8. 正交矩阵初始化
9. 单位矩阵初始化
10. 稀疏矩阵初始化
