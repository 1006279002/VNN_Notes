### 什么是优化器
PyTorch 中的优化器是用于管理并更新模型中可学习参数的值，使得模型输出更加接近真实标签

### optimizer的属性
```python
class Optimizer(object):
	def __init__(self, params, defaults):
		self.defaults = defaults
        self.state = defaultdict(dict)
        self.param_groups = []
```
- `defaults`：优化器的超参数，如 weight_decay，momentum
- `state`：参数的缓存，如 momentum 中需要用到前几次的梯度，就缓存在这个变量中
- `param_groups`：管理的参数组，是一个 list，其中每个元素是字典，包括 momentum、lr、weight_decay、params 等。
- `_step_count`：记录更新 次数，在学习率调整中使用

### optimizer的方法
* `zero_grad()`
```python
# 这是旧版，新版可以自行查看
def zero_grad(self):  
r"""Clears the gradients of all optimized :class:`torch.Tensor` s."""  
	for group in self.param_groups:  
		for p in group['params']:  
			if p.grad is not None:  
				p.grad.detach_()  
				p.grad.zero_()
```
* `step()`
* `add_param_group()`
```python
def add_param_group(self, param_group):  
	params = param_group['params']  
	if isinstance(params, torch.Tensor):  
		param_group['params'] = [params]  
	...  
	self.param_groups.append(param_group)
```
* `state_dict()`
* `load_state_dict()`
```python
def state_dict(self):  
r"""Returns the state of the optimizer as a :class:`dict`. """ 
...  
return {  
'state': packed_state,  
'param_groups': param_groups,  
}
```











