[参考博客](https://blog.csdn.net/hy592070616/article/details/120616475)
### Sigmoid函数
表达式$$Sigmoid(x)=\frac{1}{1+e^{-x}}$$
![pic2](../data/pic2.png)

### tanh函数
表达式$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
![pic3](../data/pic3.png)

### ReLU函数
表达式$$ReLU(x)=max(0,x)$$
![pic4](../data/pic4.png)

### Softmax函数
表达式$$Softmax(x)=\frac{e^{x_i}}{\sum_ie^{x_i}}$$
主要是将其归一化，同时使向量和为1，一般用于transformer机制

### Maxout函数
表达式$$Maxout(x)=max(\omega_ix_i+b_i)$$
主要是获取一组输入中的**最大值**来代表整个函数

其思路出场于论文《[Maxout Networks](https://arxiv.org/pdf/1302.4389)》

### Swish函数
表达式$$Swish(x)=x*Sigmoid(x)$$
更接近生物学的神经激活
![pic5](../data/pic5.png)
