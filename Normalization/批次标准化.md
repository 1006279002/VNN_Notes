对一个最简单的例子来说$$y=w_1x_1+w_2x_2$$如果$x_1,x_2$的数据一大一小，就会导致$\partial L/\partial w_1和\partial L/\partial w_2$变得十分不可控，可能也会一大一小，让**梯度下降**不好计算，所以需要让输入的$x_1和x_2$也有一个差不多的范围，即**标准化处理**，从而让梯度下降便于计算处理。

**特征标准化**，对于所有的输入向量$x^1,x^2,...,x^n$，取任意一个维度$i$，计算其均值$m_i$和标准差$\sigma_i$，然后进行标准化处理$$\tilde{x}_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}$$这样所有的**同维数据**其均值都为0，并且方差都为1

现在存在$n$个向量$x^1,x^2,...,x^n$，处理过程就是如下几步操作(都是维度操作，不是向量操作)
$$\begin{cases}\mu=\frac{1}{n}\sum_{i=1}^nx^i\\\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^n(x^i-\mu)^2}\\\tilde{x}^i=\frac{x^i-\mu}{\sigma}\end{cases}$$

在做feature normalization后，所有的输入向量都产生了一定的关联，所以这个输入向量是对一个**batch**进行处理的，所以是batch normalization，同时这个batch的大小一定得是**非常大**的(大数定理)

在测试的时候，如果不积累一个batch的数据，那么就无法产生对应的$\mu和\sigma$，pytorch会计算一个**moving average**$$\bar\mu \leftarrow p\bar\mu+(1-p)\mu^t,\bar\sigma\leftarrow p\bar\sigma+(1-p)\sigma^t$$其中$t$是第$t$个batch的含义，这样在新的数据进来的时候，就可以利用$\bar\mu和\bar\sigma$直接进行计算

[参考文献](https://arxiv.org/pdf/1502.03167)

[How Does Batch Normalization Help Optimization?](https://arxiv.org/pdf/1805.11604)

拓展学习
* [Batch Renormalization](https://arxiv.org/pdf/1702.03275)
* [Layer Normalization](https://arxiv.org/pdf/1607.06450)
* [Instance Normalization](https://arxiv.org/pdf/1607.08022)
* [Group Normalization](https://arxiv.org/pdf/1803.08494)
* [Weight Normalization](https://arxiv.org/pdf/1602.07868)
* [Spectrum Normalization](https://arxiv.org/pdf/1705.10941)
