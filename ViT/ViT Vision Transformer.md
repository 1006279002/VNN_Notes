ViT（Vision Transformer）是一种基于Transformer架构的视觉模型，由Google Research团队在2020年提出。它将**自然语言处理**（NLP）中成功的Transformer模型应用于计算机视觉任务，打破了传统卷积神经网络（CNN）在图像处理中的主导地位。

### 核心思想：
1. **图像分块（Patch Embedding）**  
   将输入图像分割成固定大小的非重叠块（如16x16像素），每个块展平后通过线性投影转换为向量（类似NLP中的词嵌入）。

2. **位置编码（Position Embedding）**  
   为每个图像块添加可学习的位置编码，以保留空间信息（因Transformer本身不具备空间感知能力）。

3. **Transformer编码器**  
   使用标准的Transformer结构（多头自注意力机制+前馈网络）处理图像块序列，通过全局交互捕捉长距离依赖关系。

### 关键特点：
- **无卷积操作**：完全依赖自注意力机制，适合大规模数据训练。
- **全局建模能力**：每个块能直接与其他所有块交互，克服了CNN的局部感受野限制。
- **可扩展性**：模型性能随数据量和参数量提升而稳定增长。

### 典型应用：
- 图像分类（如ImageNet）
- 目标检测（如DETR）
- 图像生成（如TransGAN）

### 优缺点：
| 优点 | 缺点 |
|------|------|
| 并行计算效率高 | 需要大量数据预训练 |
| 长距离依赖建模强 | 小数据集上易过拟合 |
| 结构统一（NLP/CV通用） | 高分辨率图像计算成本高 |

后续衍生模型如Swin Transformer通过引入层次化设计和局部窗口注意力进一步优化了计算效率。