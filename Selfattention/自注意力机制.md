self-attention的最核心[论文](https://arxiv.org/pdf/1706.03762)

当问题变成输入**一组**向量，而且这些向量的长度是**可以变化**的时候，就应该改变方法为自注意力机制

比方说对一个句子来说，将每个单词都设计为一个**向量**，然后将这一组向量作为模型的输入。
* 最简单的设计方式是**one-hot encoding**，即将向量长度设置为所有词汇的数量，然后每一个向量用其中一维置1来表示，其余维置0。但是这个处理方式不能表示相近单词之间的关系，不符合实际应用。
* 还有一种方式是**word embedding**，这种方式可以给每一个词汇一个向量，并且相近的词汇会聚集在一定的区域范围内

**输出**存在三种可能性
* 每个向量都存在一个label，即输入输出数量一一对应，可能应用于==句子中单词含义判断==(I saw a saw中每个单词的含义)
* 一整个向量序列就是一个label，可能应用于==评论检测==(正负语意判断)
* 机器自己决定输出label的输出个数，可能应用于==sequence to sequence==的模型(语句翻译)

向量和label一一对应的问题就是**sequence labeling**

self-attention可以让**所有的输入向量**互相影响后产出**新的向量**，并且输入输出个数一样。这样就可以即**和整个句子产生影响**并且**向量长度和个数可以变化**

依据某个具体向量，去其他向量中寻找和其关联的**程度**，用$\alpha$来表示。
* 最简单的计算方式是dot-product，两个输入向量分别乘上$W^q和W^k$，然后得到结果$q和k$，然后在通过计算$\alpha=q\cdot k$
* 还存在Addictive的计算方式，不过这里是将$q和k$串起来，进行$tanh$计算后再经过一个$W$乘法运算后，得到最终结果$\alpha$

向量$q$指的是query，向量$k$指的是key，对每个向量都计算其对应的$k和q$之后，就可以计算出它们之间互相对应的$\alpha$，再经过softmax激活函数得到一组$\alpha'$。然后处理原本的向量，让它们乘上一个$W^v$得到一组新的$v$，就可以计算目标向量的最终结果，比如说$$b^1=\sum_ia_{1,i}'v^i$$这样就可以从一整个sequence中得到对应的self-attention output

上面的过程可以再总结一下，这就是dot-product的**基本运算过程**
* 将每个输入向量$a_i$拼接成一个矩阵$I$，然后再与$W^q$相乘得到$q$的组合向量矩阵$Q$，所以同理可以计算剩下两个矩阵$K=W^kI,V=W^vI$。
* 然后，可以通过矩阵$Q与K$，就可以计算所有向量之间的attention关系，等式如下$$A=K^TQ$$其中$A$中的第$i$列指的是向量$i$对其他向量的attention关系，然后对$A$中的每一列做**激活函数**操作，获得矩阵$A'$(attention matrix)
* 最后得到矩阵$O$，其计算方式为$$O=VA'$$其第$i$列代表的就是最终结果$b^i$，就是self-attention的最终输出

### Multi-head Self-attention
就是在生成矩阵$Q,K,V$的基础上再将其乘上**多组不同的矩阵**，2-head就得$Q^1,Q^2,K^1,K^2,V^1,V^2$两组，3-head就有三组。然后在实际计算的时候只和**同组**的数据进行计算，最终算出来的**多组结果**将其向量相接，再和一个矩阵$W^o$相乘得到最终输出结果。

---
上述的过程没有**位置信息**，即输入向量之间的位置关系没有凸显，所有操作都是**平行操作**

使用**position encoding**的技术来给向量添上位置信息，每个位置都有一个**独特**的位置向量$e^i$，将这个向量和输入向量$a^i$进行相加就做到了position encoding操作

[参考文献](https://arxiv.org/pdf/2003.09229)

---
self-attention并不是每次都要扫描一整个句子，在attention matrix大小十分巨大的时候，说明扫描一整个句子是不好的行为，因为矩阵的运算会十分的复杂，运算效率低。这种时候可以通过**滑窗**或者**界定**的方式来选择部分向量来进行self-attention运算

self-attention也可以进行图片处理，对一张3 channel的RGB图片，可以对其**每一个像素**当成一个**3维向量**，然后进行self-attention操作，[参考文献](https://arxiv.org/pdf/1805.08318)

CNN是简化版的self-attention，self-attention是CNN的复杂版。[参考文献](https://arxiv.org/pdf/1911.03584)

self-attention的图片应用(ViT)[参考文献](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9710580&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2RvY3VtZW50Lzk3MTA1ODA=)

RNN基本被self-attention**取代**了[参考文献](https://arxiv.org/pdf/2006.16236)
* RNN操作不是平行处理的
* RNN最远的**隐藏层**很难和最开始的输入产生联系

self-attention还可以用在Graph(图)上，同时其Attention Matrix计算的过程中可以只考虑存在**edge**的地方计算attention，不存在edge则设置为0，来加强其训练效果。这是一种Graph Neural Network(GNN)

* [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/pdf/2011.04006)
* [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732)
