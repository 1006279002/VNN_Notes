### Feedforward和Recurrent对比
1. Feedforward网络不需要每一步都去输入
2. Feedforward网络每一层存在不同的参数

Feedforward中的上标$t$指代的是**层**，Recurrent中的上标$t$指代的是**时间步骤**

### 高速网络
如何将一个GRU转换为高速网络：
* 删去每一步中输入的$x^t$
* 删除每一步中输出的$y^t$
* 将隐藏层修改为上一层的输出$a^{t-1}$和这一层的输出$a^t$
* 删去reset门

这样就是在一个Feedforward NN中加入了门机制。同时微软还推出过一个Residual Network，不控制重要性，直接相加**变换结果**$h'$和其本身$a^{t-1}$


通过这样的方式，就可以控制**结果的重要性**，来控制**层的使用**(因为存在update门)

### Grid LSTM

^b3d7a3

存在**时间方向**的记忆和**深度方向**的记忆，时间方向就和原本的LSTM一致，通过$c$和$h$来控制，深度方向通过**输入并输出多个参数**的方式，同样形成记忆。

比方说**二维**深度方向通过$a$和$b$来控制，那么可以把$a$和$c$组合，$b$和$h$组合，然后获得下一个状态的数据；**三维**就可以再将新加入的参数$e$和$f$分别和$c$与$h$组合，再获得下一状态的数据
