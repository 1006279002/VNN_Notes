### Transformer遇到的最大问题
从其被提出以来，Transformer在计算attention matrix的时间复杂度问题一直被多方研究，由于矩阵乘法的本身规则，对两个$N\times d$的矩阵进行乘法计算，总共需要消耗的时间复杂度达到$O(N^2d)$，这点在**计算长文本**(类似ChatGPT的长对话)上是十分消耗算力的。

### RNN是什么东西
[[../RNN/Recurrent Structure(RNN)|Recurrent Structure(RNN)]]里面已经详细介绍了其原理，我们都知道RNN能够长距离记忆过去的输入，但是它主要存在两个问题。
* 随着时间的推移，RNN往往会忘记很久之前发生的事情，即不能远距离产生影响
* RNN无法并行训练，每个输出都是依赖过去的输出，与self-attention差距明显

### 所以，什么是SSM？
SSM(State Space Models)主要源自`卡尔曼滤波器`，下面一一具体介绍
#### 状态空间
假设存在一个迷宫需要我们穿越，其是一个**二维平面**，每一个位置都可以用一个坐标来表示，并且存在一个**隐藏的信息**，就是我们离出口的距离。

这个迷宫可以通过**状态空间**来进行描述
* 当前所在的状态(起点在哪)
* 潜在的下一个状态(下一步可能去哪)
* 通过什么样的函数去往下一个状态(如何移动)

而这些**状态**可以用**状态向量**来描述，比方说上述示例就是一个三维向量$(x,y,d)$
* $x$代表其横坐标位置
* $y$代表其纵坐标位置
* $d$代表其离出口的距离
#### SSM本质就是一个输入不离散的RNN
一般，SSM包含下面的几个组成
* 映射输入序列$x(t)$，比如移动方向
* 潜在状态表示$h(t)$，比如距离出口的距离和坐标
* 导出预测序列$y(t)$，比如往哪里移动可以更快到达出口

同时，它是一组**连续**序列作为输入并预测输出序列，其表达式如下$$h'(t)=Ah(t)+Bx(t)\ \text{State equation}$$$$y(t)=Ch(t)+Dx(t)\ \text{Outpuit equation}$$

这个表达式和RNN中的$h^t=tanh(Wh^{t-1}+Ux^t)$十分类似，但是由于输入是连续的，所以表达式不存在$t-1$，其中系数$A$是**存储之前历史信息的矩阵**，其决定了下一个时刻的**空间状态**
#### 状态方程和输出方程
由上文可知，SSM最重要的关键就是定义好其**状态**$h(t)$的表示方法，在最初的SSM中，学习到的$A,B,C,D$四个参数就**不会再发生变化了**

其实际结构可以简化如下<br>
![SSM示意图](../Excalidraw/SSM)

SSM主要的核心还是两个**连续sequence**的互相影响，其中$D$可以看成类似**残差设计**中的跳跃机制，所以真正的核心就是通过矩阵$A,B,C$组成的结构
