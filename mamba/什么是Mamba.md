关于Mamba模型的学习[参考博客](https://blog.csdn.net/v_JULY_v/article/details/134923301)

自Transformer提出以来，有不少模型想要与其对抗争锋，**Mamba**也是其中之一，下面仔细介绍一下Mamba。

首先，对于Mamba的由来，需要按照一定的分块来学习

### Part 1 SSM核心
[[什么是SSM]]

### Part 2 SSM的升级
[[SSM到S4及S4D的进化]]

### Part 3 Mamba的提出
介绍完了上述两部分内容，就要开始说明Mamba是什么了，其核心就是**有选择处理信息+硬件感知算法+更简单的SSM架构**

#### 选择性状态空间模型
作者认为，序列建模的问题就是把上下文变成更小的状态，由此可以看出
* Transformer效果很好但是**效率不高**，KV缓存使其训练和推理消耗很大
* RNN推理和训练效率虽然高，但是性能容易受到上下文压缩程度的影响，即虽然快但是容易**忘掉前面的内容**
* SSM因为$A,B,C$矩阵固定导致无法对**不同的输入针对性做出推理**
* 所以Mamba采用了**参数化SSM的输入**，选择性处理输入信息

如下表总结

| 模型                 | 对信息的压缩程度               | 训练的效率                           | 推理的效率                         |
| ------------------ | ---------------------- | ------------------------------- | ----------------------------- |
| transformer(注意力机制) | transformer对每个历史记录都不压缩 | 训练消耗算力大                         | 推理消耗算力大                       |
| RNN                | 随着时间的推移，RNN往往会忘记某一部分信息 | RNN无法并行训练                       | 推理时只看一个时间步，故推理高效(相当于推理快但是训练慢) |
| CNN                |                        | 训练效率高，可以并行                      |                               |
| SSM                | SSM压缩每一个历史记录           |                                 | 矩阵不因输入不同而不同，无法针对输入针对性推理       |
| Mamba              | 选择性关注必须关注的，过滤掉可以忽略的    | Mamba每次参考前面所有内容的一个概括，兼具训练、推理的效率 |                               |

首先Mamba对输入的**每个维度**(比方说图片的三通道就是三个维度)都使用了一个独立的SSM，类似Transformer中多头的独立操作

然后将原本SSM4(**Algorithm 1**)的算法改写为如下图右边所示(==图片引用顶端参考博客==)<br>
![[../data/pic8.png]]
* 输入$x$中，参数$\text{B}$指的是**批量大小**，参数$\text{L}$指的是**序列长度**，参数$\text{D}$指的是**维度**
* 矩阵$A$中的参数$\text{N}$指的是**隐藏层状态大小**
* 后续同理

首先变化比较明显的是矩阵$B,C$和步长$\Delta$的变化，首先矩阵$B和C$变成了$(\text{B},\text{L},\text{N})$，分别指代批量大小，序列长度和**隐藏层状态大小**。而$\Delta$的变化则是意味着对于每一个不同的batch中的每一个token都存在一个**步长**。整体表达了，对于每一个不同的token都存在不同的参数。并且，在test阶段，虽然**模型自带的参数**是不变的，但是在test阶段会**应用训练中学到的方式**(并非重新学一遍)，对这些参数进行**动态调整**

具体的调整方式是通过一个**线性层映射**，将输入映射成对应的$B,C和\Delta$

通过步长$\Delta$来判断参数是否**重要**，如果步长小就**忽略**，步长大就**重点关注**

#### 硬件感知的设计:并行扫描且借鉴Flash Attention
因为矩阵的**可变化性**，无法通过卷积的方式来计算训练，所以为了实现并行化，就采取了**并行扫描**

原本正常计算，是需要计算出上一个状态的隐藏层才能计算下一个状态，这样就无法实现并行。

采用**选择性扫描算法**假设执行操作的顺序与关联属性无关，分段计算序列并迭代组合它们

#### 简化的SSM架构及最终的整体流程
将大多数SSM架构的基础快与**现代神经网络**(比如Gated MLP)结合，组成Mamba快，重复此块就形成了Mamba架构(下图同样出自顶部文章)<br>
![[../data/pic9.png]]

* 做线性投影：增加输入嵌入的维度，处理更高维度的特征空间(比较像**倒残差**)
* 卷积的作用：预处理数据
	* 提取局部特征
	* 建立token的**局部上下文关系**，防止独立token计算
