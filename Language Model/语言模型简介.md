什么是Language Model？估测**一句话的几率**

现在存在一个**词语sequence**:$w_1,w_2,...,w_n$，那么这个模型就是要计算句子出现的几率$P(w_1,w_2,...,w_n)$

### N-gram
2-gram(只考虑前一个词)可以把整个句子的出现概率用下面的表达式来表示$$P(w_1,w_2,...,w_n)=P(w_1|START)P(w_2|w_1)...P(w_n|w_{n-1})$$
而其中的每一部分可以通过训练集来完成，例如($C$指的是统计出现次数)$$P(beach|rice)=\frac{C(nice \ beach)}{C(rice)}$$
在$n$很大/很小的时候，容易**估计不准确单词**的几率。利用**smoothing**给一个比较小的几率，而非给极端情况(0/1)
### NN-based LM
核心便是输入词$a$，输出词$b$出现的几率，然后把每个几率都乘起来便是生成出这句话的几率。

### RNN-based LM
会记忆每个过程中的输出，综合上文来判断下一个词出现的几率

### Matrix Factorization 矩阵分解
通过**训练集**，将词汇之间的前后关系概率填入矩阵，然后，将每个词汇都量化为**相同维度**的向量，利用Gradient Descent去重新填写整个矩阵，计算出训练集中**不存在的**的概率情况

|         | dog | cat | ... | child |
| ------- | --- | --- | --- | ----- |
| ran     | 0.2 | 0.3 |     | 0.1   |
| jumped  | 0   | 0.2 |     | 0.1   |
| cried   | 0   | 0   |     | 0.3   |
| laughed | 0   | 0   |     | 0.3   |
| ...     |     |     |     |       |
通过最小化$$L=\sum_{(i,j)}(v^i\cdot h^j-n_{ij})^2$$来训练每个单词的**向量数值**，最后再去补充表格中每个数据，例如$n_{12}=v^1h^2$

这样可以在训练的时候将**接近的单词**的向量处理，cat后接jumped可能性大的话，dog后面也有很大可能接jumped

在进行Gradient Descent的过程中，使用NN思想，利用softmax激活函数将每个计算结果进行概率归一化，使结果直接为两个单词前后关系成立的概率。然后在对其做**交叉熵计算**训练每一个词汇向量

---
利用RNN，可以更大量地减少参数的数量
