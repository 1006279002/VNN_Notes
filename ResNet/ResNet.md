ResNet（Residual Neural Network，残差神经网络）是由微软研究院的何恺明等人于2015年提出的深度卷积神经网络架构，其核心创新是**残差学习**（Residual Learning），解决了深度神经网络训练中的梯度消失/爆炸问题，使得网络可以极深度化（如超过1000层）。

---

### **1. 核心思想：残差块（Residual Block）**
- **问题背景**：传统深度网络随着层数增加，训练误差反而上升（**退化问题**），并非由过拟合引起，而是梯度难以有效反向传播。
- **解决方案**：引入“跳跃连接”（Shortcut Connection），将输入直接加到输出上，形成残差结构：
  $$ y = F(x) + x $$
  其中：
  - $x$：输入
  - $F(x)$：残差函数（需学习的部分）
  - $y$：输出
- **优势**：网络只需学习输入与输出的差异（残差），简化了优化目标。

---

### **2. 网络结构**
#### **残差块类型**
- **Basic Block**：用于浅层网络（如ResNet-18/34），包含两个3×3卷积层。
```plaintext
  [Conv3×3] → [BN] → [ReLU] → [Conv3×3] → [BN] → [Add] → [ReLU]
```
- **Bottleneck Block**：用于深层网络（如ResNet-50及以上），通过1×1卷积**降维和升维**减少计算量：
```plaintext
  [Conv1×1] → [BN] → [ReLU] → [Conv3×3] → [BN] → [ReLU] → [Conv1×1] → [BN] → [Add] → [ReLU]
```

#### **经典变体**
| 模型         | 层数 | 残差块类型       | 参数量（百万） |
|--------------|------|------------------|----------------|
| ResNet-18    | 18   | Basic Block       | ~11            |
| ResNet-34    | 34   | Basic Block       | ~21            |
| ResNet-50    | 50   | Bottleneck Block  | ~25            |
| ResNet-101   | 101  | Bottleneck Block  | ~44            |
| ResNet-152   | 152  | Bottleneck Block  | ~60            |

---

### **3. 关键创新点**
- **跳跃连接的实现**：
  - 当输入输出维度一致时，直接相加（Identity Shortcut）。
  - 维度不一致时，通过1×1卷积调整维度（Projection Shortcut）。
- **梯度传播优化**：残差结构使梯度可直接回传到底层，缓解梯度消失。
- **网络深度灵活性**：支持从几十层到上千层的扩展（如ResNet-1202）。

---

### **4. 应用场景**
- **图像分类**（如ImageNet竞赛）
- **目标检测**（如Faster R-CNN的骨干网络）
- **语义分割**（如PSPNet）
- **其他领域**：视频分析、生成模型（如ResNet-GAN）等。

---

### **5. 优势与局限性**
- **优势**：
  - 训练更深的网络而不退化。
  - 参数效率高，性能优于VGG等传统网络。
  - 易于迁移学习。
- **局限性**：
  - 深层ResNet可能过拟合小数据集。
  - 跳跃连接增加内存占用。

---

### **6. 后续改进**
- **ResNeXt**：引入分组卷积（Grouped Convolution）提升效率。
- **Wide ResNet**：增加通道数而非深度。
- **Res2Net**：多尺度残差结构。
